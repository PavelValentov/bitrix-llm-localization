# Translation backend: "api" (OpenAI) or "local" (LM Studio)
# TRANSLATION_BACKEND=api

# Required Languages (comma-separated)
# Default: ru,en,ua,tr (set in run_translation.sh if not specified here)
# Override to translate only specific languages:
# REQUIRED_LANGS=en,tr

# --- API backend (requires TRANSLATION_BACKEND=api) ---
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Optional: OpenAI Model (default: gpt-4o)
# OPENAI_MODEL=gpt-4o

# Optional: Base URL (if using a proxy or compatible API)
# OPENAI_BASE_URL=https://api.openai.com/v1

# --- Local backend (TRANSLATION_BACKEND=local) ---
# LM Studio: Start the local server, then set:
#   TRANSLATION_BACKEND=local
#   LOCAL_LLM_URL=http://localhost:1234/v1
#   LOCAL_LLM_MODEL=gpt-oss-20b-MLX-8bit
# TRANSLATION_BACKEND=local
# LOCAL_LLM_URL=http://localhost:1234/v1
# LOCAL_LLM_MODEL=gpt-oss-20b-MLX-8bit

# --- Local MLX server (TRANSLATION_BACKEND=local-server) ---
# Our Python mlx-server: loads model, supports /reload every N batches
# Model: Qwen3-8B (text-only, 8-bit) â€” better for translation than Qwen3-VL-8B
# 1. Start: python mlx-server/mlx_server.py -m /path/to/model -p 8765
# 2. Set: TRANSLATION_BACKEND=local-server
# TRANSLATION_BACKEND=local-server
# LOCAL_SERVER_URL=http://127.0.0.1:8765
# LOCAL_SERVER_RELOAD_EVERY_BATCHES=10
# MLX_MODEL_PATH=~/.lmstudio/models/lmstudio-community/Qwen3-8B-MLX-8bit

# Enable model thinking/reasoning mode (Qwen uses <think> tags)
# false = direct translation (faster, cleaner output) - DEFAULT
# true = model shows reasoning process (slower, may need post-processing)
# ENABLE_MODEL_THINKING=false

# Maximum tokens in model response (default: 16384)
# Increase if you have very large batches or long strings
# Dynamic batching automatically fills up to half of this value for optimal throughput
# MAX_RESPONSE_TOKENS=16384

# Hugging Face token (optional, for faster NLLB model downloads)
# Create at: https://huggingface.co/settings/tokens
HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
