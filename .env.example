# =============================================================================
# Bitrix24 Localization â€” Environment Configuration
# Copy to .env and set your values. Only one TRANSLATION_BACKEND is active.
# =============================================================================

# --- Backend choice (uncomment one) ---
# api          = OpenAI or compatible API
# local        = LM Studio (localhost)
# local-server = MLX server (run_translation.sh or python mlx_server.py)
TRANSLATION_BACKEND=local-server

# --- API backend (when TRANSLATION_BACKEND=api) ---
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# Optional: model (default: gpt-4o)
OPENAI_MODEL=gpt-4o
# Optional: base URL (proxy or compatible API, e.g. OpenAI-compatible)
OPENAI_BASE_URL=https://api.openai.com/v1

# --- Local LM Studio (when TRANSLATION_BACKEND=local) ---
# LOCAL_LLM_URL=http://localhost:1234/v1
# LOCAL_LLM_MODEL=gpt-oss-20b-MLX-8bit

# --- Local MLX server (when TRANSLATION_BACKEND=local-server) ---
# Used by pnpm translate and run_translation.sh
LOCAL_SERVER_URL=http://127.0.0.1:8765
# Reload model every N batches (0 = no reload)
LOCAL_SERVER_RELOAD_EVERY_BATCHES=10
# Model path for run_translation.sh (optional; default: ~/.lmstudio/models/.../Qwen3-8B-MLX-8bit)
# MLX_MODEL_PATH=~/.lmstudio/models/lmstudio-community/Qwen3-8B-MLX-8bit
# Port override for MLX server (optional; default: 8765)
# LOCAL_SERVER_PORT=8765

# --- Translation limits ---
# Max tokens in model response (default: 16384)
MAX_RESPONSE_TOKENS=16384
# Enable model "thinking" / reasoning (Qwen <think> tags). false = faster, cleaner (default)
ENABLE_MODEL_THINKING=false

# --- Languages to fill (comma-separated) ---
# Used by run_translation.sh and pnpm translate --required=...
# Script default if unset: ru,tr,en
REQUIRED_LANGS=en,tr,ru

# --- Optional: Hugging Face (e.g. NLLB / other tools) ---
# HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
